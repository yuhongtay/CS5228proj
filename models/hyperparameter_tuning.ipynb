{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617a172d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55fcf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = [\"FLAT_TYPE\", \"FLAT_MODEL\", \"SUBZONE\", \"TOWN\"]\n",
    "\n",
    "def find_best_parameter(X, y):\n",
    "    numerical_features = X.select_dtypes(include=np.number).columns.tolist()\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', 'passthrough', numerical_features),\n",
    "            ('cat', OneHotEncoder(handle_unknown='ignore'), cat_features)\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "\n",
    "    def objective(trial, model_name, X_data, y_data):\n",
    "        if model_name == \"LightGBM\":\n",
    "            params = {\n",
    "                'objective': 'regression_l1',\n",
    "                'metric': 'rmse',\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "                'num_leaves': trial.suggest_int('num_leaves', 20, 300),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "                'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "                'random_state': 42,\n",
    "                \"verbosity\": -1,\n",
    "                'n_jobs': -1\n",
    "            }\n",
    "            model = lgb.LGBMRegressor(**params)\n",
    "\n",
    "        elif model_name == \"XGBoost\":\n",
    "            params = {\n",
    "                'objective': 'reg:squarederror',\n",
    "                'eval_metric': 'rmse',\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "                'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "                'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "                'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),\n",
    "                'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),\n",
    "                'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "                'random_state': 42,\n",
    "                'n_jobs': -1\n",
    "            }\n",
    "            model = xgb.XGBRegressor(**params)\n",
    "    \n",
    "        elif model_name == \"RandomForest\":\n",
    "            params = {\n",
    "                'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "                'max_depth': trial.suggest_int('max_depth', 5, 50),\n",
    "                'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "                'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 20),\n",
    "                'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2']),\n",
    "                'random_state': 42,\n",
    "                'n_jobs': -1\n",
    "            }\n",
    "            model = RandomForestRegressor(**params)\n",
    "    \n",
    "        elif model_name == \"Ridge\":\n",
    "            params = {\n",
    "                'alpha': trial.suggest_float('alpha', 1e-5, 100.0, log=True),\n",
    "                'random_state': 42\n",
    "            }\n",
    "            model = Ridge(**params)\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Model {model_name} is not supported for tuning.\")\n",
    "\n",
    "        pipeline = Pipeline(steps=[('preprocessor', preprocessor), ('regressor', model)])\n",
    "        score = cross_val_score(pipeline, X_data, y_data, n_jobs=-1, cv=3, scoring='neg_root_mean_squared_error').mean()\n",
    "    \n",
    "        return -score\n",
    "    \n",
    "    models_to_tune = [\"LightGBM\", \"XGBoost\", \"RandomForest\", \"Ridge\"]\n",
    "    best_params_all_models = {}\n",
    "    n_trials_per_model = 50\n",
    "\n",
    "    for model_name in models_to_tune:\n",
    "        print(f\"Starting tuning for {model_name}\")\n",
    "        study = optuna.create_study(direction=\"minimize\")\n",
    "        study.optimize(lambda trial: objective(trial, model_name, X, y), n_trials=n_trials_per_model)\n",
    "        print(f\"Finished tuning {model_name}\")\n",
    "        best_params_all_models[model_name] = {\n",
    "            \"best_value\": study.best_value,\n",
    "            \"best_params\": study.best_params,\n",
    "        }\n",
    "    \n",
    "    return best_params_all_models"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
